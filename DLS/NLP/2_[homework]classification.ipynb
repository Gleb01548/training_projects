{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot3c4fjZwC4T"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2JdzEXmwRU5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TItJgQMp5p4E"
   },
   "source": [
    "# Задание 3\n",
    "\n",
    "## Классификация текстов\n",
    "\n",
    "В этом задании вам предстоит попробовать несколько методов, используемых в задаче классификации, а также понять насколько хорошо модель понимает смысл слов и какие слова в примере влияют на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1emhHMhniZMu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyOlPZA26Ppy"
   },
   "source": [
    "В этом задании мы будем использовать библиотеку torchtext. Она довольна проста в использовании и поможет нам сконцентрироваться на задаче, а не на написании Dataloader-а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SBK4ipzS122j"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNWLG7mG6n2d"
   },
   "source": [
    "Датасет на котором мы будем проводить эксперементы это комментарии к фильмам из сайта IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mRzUSWeAi6Xq"
   },
   "outputs": [],
   "source": [
    "train = IMDB('data\\datasets', split= 'train')\n",
    "test = IMDB('data\\datasets', split= 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - негативный, 2 - позитивный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "\n",
    "train_list = []\n",
    "counter_line = Counter()\n",
    "counter_label = Counter()\n",
    "len_list = []\n",
    "for label, line in train:\n",
    "    label -= 1\n",
    "    tokenz = tokenizer(line)\n",
    "    len_list.append(len(tokenz))\n",
    "    \n",
    "    counter_label.update(str(label))\n",
    "    counter_line.update(tokenz)\n",
    "    train_list.append((label, tokenz))\n",
    "    \n",
    "    \n",
    "index_word = {word:ind+2 for ind, word in enumerate(counter_line.keys())}\n",
    "index_word['unknow'] = 1\n",
    "random.shuffle(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100683"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100682"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = []\n",
    "for label, line in test:\n",
    "    label -= 1\n",
    "    tokenz = tokenizer(line)\n",
    "    val_list.append((label, tokenz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 12500, '1': 12500})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab = pd.Series(len_list)\n",
    "tab.hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292.0\n",
      "374.0\n",
      "530.0\n"
     ]
    }
   ],
   "source": [
    "print(tab.quantile(0.7))\n",
    "print(tab.quantile(0.8))\n",
    "print(tab.quantile(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for label, stri in list_result:\n",
    "        stri = list(map(lambda x: index_dict.get(x, 1), stri[:LEN_STRI]))\n",
    "        stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "        data.append((label, torch.LongTensor(stri))\n",
    "                   )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_STRI = 292\n",
    "train_data = trans_string(train_list, index_word, LEN_STRI)\n",
    "test_data = trans_string(val_list, index_word, LEN_STRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_CRDES360wG"
   },
   "source": [
    "## RNN\n",
    "\n",
    "Для начала попробуем использовать рекурентные нейронные сети. На семинаре вы познакомились с GRU, вы можете также попробовать LSTM. Можно использовать для классификации как hidden_state, так и output последнего токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "J1yE1KPQqDat"
   },
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           n_layers, bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(), \n",
    "                                nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "                                nn.Linear(int(hidden_dim/2), output_dim)\n",
    "                               )\n",
    "        self.flat = nn.Flatten(0, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "       #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        # cell arg for LSTM, remove for GRU\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #unpack sequence\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)  \n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        #hidden = None  # YOUR CODE GOES HERE\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions] or [batch_size, hid dim * num directions]\n",
    "        #print(hidden.shape)\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        fc = self.fc(hidden)\n",
    "            \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7dLGFg4M7Te"
   },
   "source": [
    "Поиграйтесь с гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ggTiORJ-8t0J"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(index_word) + 1\n",
    "emb_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "bidirectional = False\n",
    "dropout = 0.2\n",
    "#PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "patience=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mIiM_ZBt9_91"
   },
   "outputs": [],
   "source": [
    "model = RNNBaseline(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    n_layers=n_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout=0,\n",
    "    pad_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mFeG88M--NbD"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "olAS-mVI-VfT"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jlfEAJu9akO"
   },
   "source": [
    "Обучите сетку! Используйте любые вам удобные инструменты, Catalyst, PyTorch Lightning или свои велосипеды."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "87dgw6ok9hR0"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "cur_patience = 0\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar: \n",
    "        #YOUR CODE GOES HERE\n",
    "\n",
    "    train_loss /= len(train_iter)\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar:\n",
    "        # YOUR CODE GOES HERE\n",
    "    val_loss /= len(valid_iter)\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        cur_patience += 1\n",
    "        if cur_patience == patience:\n",
    "            cur_patience = 0\n",
    "            break\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    metric_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (y, x) in enumerate(train_loader):\n",
    "\n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)  \n",
    "            \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "            print(i_step,  end='\\r')\n",
    "        \n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_metrics = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        metric_history.append(val_metrics)\n",
    "        \n",
    "        print(f'Average loss:{ave_loss}, f1: {val_metrics[2]}, \\\n",
    "              precision: {val_metrics[0]}, recall: {val_metrics[1]}')\n",
    "        #print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, metric_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    predict_list = []\n",
    "    y_list = []\n",
    "    model.eval() # Evaluation mode\n",
    "    for y, X in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = model(X)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        predict_list.extend(list(index.cpu()))\n",
    "        y_list.extend(list(y.cpu()))\n",
    "    \n",
    "    precision = precision_score(y_list, predict_list)\n",
    "    recall = recall_score(y_list, predict_list)\n",
    "    f1 = f1_score(y_list, predict_list)\n",
    "    \n",
    "    return (precision, recall, f1)\n",
    "    raise Exception(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "#loss_func = nn.BCEWithLogitsLoss()\n",
    "loss_func = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:0.6950721144676208, f1: 0.6291436672397671,               precision: 0.5049629593763618, recall: 0.83432\n",
      "Average loss:0.6935364603996277, f1: 0.6536593726789693,               precision: 0.5083533280014219, recall: 0.91528\n",
      "Average loss:0.6803185939788818, f1: 0.2741708049782356,               precision: 0.5867226449750722, recall: 0.17888\n",
      "Average loss:0.6625267267227173, f1: 0.2828210198334232,               precision: 0.5633802816901409, recall: 0.1888\n",
      "Average loss:0.6196848154067993, f1: 0.6550549034456645,               precision: 0.5150460312371181, recall: 0.8996\n",
      "Average loss:0.5505903959274292, f1: 0.38103213650162254,               precision: 0.5510142294883439, recall: 0.2912\n",
      "Average loss:0.4012276828289032, f1: 0.7557287475115423,               precision: 0.803042578089837, recall: 0.71368\n",
      "Average loss:0.2492179572582245, f1: 0.8046202998279676,               precision: 0.8244082591908679, recall: 0.78576\n",
      "Average loss:0.14444269239902496, f1: 0.82164297610287,               precision: 0.7963811096929199, recall: 0.84856\n",
      "Average loss:0.08422444015741348, f1: 0.826964395334561,               precision: 0.79452963727514, recall: 0.86216\n",
      "Average loss:0.044230785220861435, f1: 0.8025579536370904,               precision: 0.8463927588960866, recall: 0.76304\n",
      "Average loss:0.02597063034772873, f1: 0.8079107587954072,               precision: 0.8256911383947214, recall: 0.79088\n",
      "Average loss:0.020725129172205925, f1: 0.8080271996019571,               precision: 0.8386985711826476, recall: 0.77952\n",
      "Average loss:0.01424580067396164, f1: 0.8313618216767706,               precision: 0.7727460142935679, recall: 0.8996\n",
      "Average loss:0.018878603354096413, f1: 0.799813859040528,               precision: 0.8487161070210092, recall: 0.75624\n",
      "Average loss:0.010268268175423145, f1: 0.8043496375302057,               precision: 0.83924534863502, recall: 0.77224\n",
      "Average loss:0.01391552109271288, f1: 0.819725324479324,               precision: 0.775706940874036, recall: 0.86904\n",
      "Average loss:0.007908664643764496, f1: 0.8170034154973046,               precision: 0.8411998983137022, recall: 0.79416\n",
      "Average loss:0.007589452899992466, f1: 0.8266452648475121,               precision: 0.8293075684380032, recall: 0.824\n",
      "Average loss:0.009027808904647827, f1: 0.8195923577041203,               precision: 0.8172778617452868, recall: 0.82192\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, metric_history \\\n",
    "= train_model(model, train_loader, val_loader, loss_func, opt, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4i-Go_ICT_U"
   },
   "source": [
    "Посчитайте f1-score вашего классификатора на тестовом датасете.\n",
    "\n",
    "**Ответ**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWkbCpNECflR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzDqc3__JIMe"
   },
   "source": [
    "## CNN\n",
    "\n",
    "![](https://www.researchgate.net/publication/333752473/figure/fig1/AS:769346934673412@1560438011375/Standard-CNN-on-text-classification.png)\n",
    "\n",
    "Для классификации текстов также часто используют сверточные нейронные сети. Идея в том, что как правило сентимент содержат словосочетания из двух-трех слов, например \"очень хороший фильм\" или \"невероятная скука\". Проходясь сверткой по этим словам мы получим какой-то большой скор и выхватим его с помощью MaxPool. Далее идет обычная полносвязная сетка. Важный момент: свертки применяются не последовательно, а параллельно. Давайте попробуем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU-76tNI-STt"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, batch_first=True)  # batch_first тк мы используем conv  \n",
    "LABEL = LabelField(batch_first=True, dtype=torch.float)\n",
    "\n",
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split(random_state=random.seed(SEED))\n",
    "\n",
    "TEXT.build_vocab(trn)\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQpS9KKUJQVH"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(128, 256, 256),\n",
    "        sort=False,\n",
    "        sort_key= lambda x: len(x.src),\n",
    "        sort_within_batch=False,\n",
    "        device=device,\n",
    "        repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asgbwMePPNNl"
   },
   "source": [
    "Вы можете использовать Conv2d с `in_channels=1, kernel_size=(kernel_sizes[0], emb_dim))` или Conv1d c `in_channels=emb_dim, kernel_size=kernel_size[0]`. Но хорошенько подумайте над shape в обоих случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPP_-0E-JYTQ"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim,\n",
    "        out_channels,\n",
    "        kernel_sizes,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv_0 = None  # YOUR CODE GOES HERE\n",
    "        \n",
    "        self.conv_1 = None  # YOUR CODE GOES HERE\n",
    "        \n",
    "        self.conv_2 = None  # YOUR CODE GOES HERE\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        embedded = embedded  # may be reshape here\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here\n",
    "        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here\n",
    "        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-U_2T5oKNed"
   },
   "outputs": [],
   "source": [
    "kernel_sizes = [3, 4, 5]\n",
    "vocab_size = len(TEXT.vocab)\n",
    "out_channels=64\n",
    "dropout = 0.5\n",
    "dim = 300\n",
    "\n",
    "model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,\n",
    "            kernel_sizes=kernel_sizes, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC2ThnfNKPIR"
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mExblVtPKRw4"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVwSgwkEKTw5"
   },
   "outputs": [],
   "source": [
    "max_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIQgLCELDoOA"
   },
   "source": [
    "Обучите!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQZbJ791KXHb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "cur_patience = 0\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar: \n",
    "        #YOUR CODE GOES HERE\n",
    "\n",
    "    train_loss /= len(train_iter)\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar:\n",
    "        # YOUR CODE GOES HERE\n",
    "    val_loss /= len(valid_iter)\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        cur_patience += 1\n",
    "        if cur_patience == patience:\n",
    "            cur_patience = 0\n",
    "            break\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UVCacK0EhPR"
   },
   "source": [
    "Посчитайте f1-score вашего классификатора.\n",
    "\n",
    "**Ответ**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VspGMN0ESiS"
   },
   "source": [
    "## Интерпретируемость\n",
    "\n",
    "Посмотрим, куда смотрит наша модель. Достаточно запустить код ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ye2SvjXrPgJh"
   },
   "outputs": [],
   "source": [
    "!pip install -q captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e5XPKSZO6DY"
   },
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "PAD_IND = TEXT.vocab.stoi['pad']\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
    "lig = LayerIntegratedGradients(model, model.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvqWhd-fPe9e"
   },
   "outputs": [],
   "source": [
    "def forward_with_softmax(inp):\n",
    "    logits = model(inp)\n",
    "    return torch.softmax(logits, 0)[0][1]\n",
    "\n",
    "def forward_with_sigmoid(input):\n",
    "    return torch.sigmoid(model(input))\n",
    "\n",
    "\n",
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model, sentence, min_len = 7, label = 0):\n",
    "    model.eval()\n",
    "    text = [tok for tok in TEXT.tokenize(sentence)]\n",
    "    if len(text) < min_len:\n",
    "        text += ['pad'] * (min_len - len(text))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in text]\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    input_indices = torch.tensor(indexed, device=device)\n",
    "    input_indices = input_indices.unsqueeze(0)\n",
    "    \n",
    "    # input_indices dim: [sequence_length]\n",
    "    seq_length = min_len\n",
    "\n",
    "    # predict\n",
    "    pred = forward_with_sigmoid(input_indices).item()\n",
    "    pred_ind = round(pred)\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
    "                                           n_steps=5000, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', LABEL.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            LABEL.vocab.itos[pred_ind],\n",
    "                            LABEL.vocab.itos[label],\n",
    "                            LABEL.vocab.itos[1],\n",
    "                            attributions.sum(),       \n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtYy633vS8Me"
   },
   "outputs": [],
   "source": [
    "interpret_sentence(model, 'It was a fantastic performance !', label=1)\n",
    "interpret_sentence(model, 'Best film ever', label=1)\n",
    "interpret_sentence(model, 'Such a great show!', label=1)\n",
    "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
    "interpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\n",
    "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqIRSCWlRTOe"
   },
   "source": [
    "Попробуйте добавить свои примеры!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4URAkcWXTGBi"
   },
   "outputs": [],
   "source": [
    "print('Visualize attributions based on Integrated Gradients')\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvEHEaurElu8"
   },
   "source": [
    "## Эмбеддинги слов\n",
    "\n",
    "Вы ведь не забыли, как мы можем применить знания о word2vec и GloVe. Давайте попробуем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW46gGLNuo0q"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, vectors=)# YOUR CODE GOES HERE\n",
    "# подсказка: один из импортов пока не использовался, быть может он нужен в строке выше :)\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "kernel_sizes = [3, 4, 5]\n",
    "vocab_size = len(TEXT.vocab)\n",
    "dropout = 0.5\n",
    "dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ4YwLlcltm3"
   },
   "outputs": [],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split(random_state=random.seed(SEED))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return self.fc(hidden)\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(128, 256, 256),\n",
    "        sort=False,\n",
    "        sort_key= lambda x: len(x.src),\n",
    "        sort_within_batch=False,\n",
    "        device=device,\n",
    "        repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l5pDvZgl7Fp"
   },
   "outputs": [],
   "source": [
    "model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=64,\n",
    "            kernel_sizes=kernel_sizes, dropout=dropout)\n",
    "\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "prev_shape = model.embedding.weight.shape\n",
    "\n",
    "model.embedding.weight = # инициализируйте эмбэдинги\n",
    "\n",
    "assert prev_shape == model.embedding.weight.shape\n",
    "model.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwiQjcuiFGTC"
   },
   "source": [
    "Вы знаете, что делать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNqcFHT8cT0b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "cur_patience = 0\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar: \n",
    "        #YOUR CODE GOES HERE\n",
    "\n",
    "    train_loss /= len(train_iter)\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    for it, batch in pbar:\n",
    "        # YOUR CODE GOES HERE\n",
    "    val_loss /= len(valid_iter)\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        cur_patience += 1\n",
    "        if cur_patience == patience:\n",
    "            cur_patience = 0\n",
    "            break\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2IdEWJQKESg"
   },
   "source": [
    "Посчитайте f1-score вашего классификатора.\n",
    "\n",
    "**Ответ**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kizk028eRF0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sl7h_wIRGPD"
   },
   "source": [
    "Проверим насколько все хорошо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPCm55FLir3e"
   },
   "outputs": [],
   "source": [
    "PAD_IND = TEXT.vocab.stoi['pad']\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
    "lig = LayerIntegratedGradients(model, model.embedding)\n",
    "vis_data_records_ig = []\n",
    "\n",
    "interpret_sentence(model, 'It was a fantastic performance !', label=1)\n",
    "interpret_sentence(model, 'Best film ever', label=1)\n",
    "interpret_sentence(model, 'Such a great show!', label=1)\n",
    "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
    "interpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\n",
    "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMDazB3AlFWA"
   },
   "outputs": [],
   "source": [
    "print('Visualize attributions based on Integrated Gradients')\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flCZHdAVlL7W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " [homework]classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
